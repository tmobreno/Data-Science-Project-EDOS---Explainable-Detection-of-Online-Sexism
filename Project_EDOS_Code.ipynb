{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cb06b4-f58d-46e0-8f71-b616bcc0512d",
   "metadata": {},
   "source": [
    "## A Introduction to the competition\n",
    "\n",
    "<img src=\"news-sexisme-EN.jpg\" alt=\"drawing\" width=\"380\"/>\n",
    "\n",
    "Sexism is a growing problem online. It can inflict harm on women who are targeted, make online spaces inaccessible and unwelcoming, and perpetuate social asymmetries and injustices. Automated tools are now widely deployed to find, and assess sexist content at scale but most only give classifications for generic, high-level categories, with no further explanation. Flagging what is sexist content and also explaining why it is sexist improves interpretability, trust and understanding of the decisions that automated tools use, empowering both users and moderators.\n",
    "\n",
    "This project is based on SemEval 2023 - Task 10 - Explainable Detection of Online Sexism (EDOS). [Here](https://codalab.lisn.upsaclay.fr/competitions/7124#learn_the_details-overview) you can find a detailed introduction to this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING IN THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7caf28-1c77-4f87-8a62-7f3a7f79e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Loading\n",
    "\n",
    "# read in the data\n",
    "message_df = pd.read_csv('edos_labelled_data.csv')\n",
    "\n",
    "# split the data into train and test\n",
    "train_data = message_df[message_df['split'] == 'train']\n",
    "test_data = message_df[message_df['split'] == 'test']\n",
    "\n",
    "# store train x and y data\n",
    "X_train = train_data['text']\n",
    "y_train = train_data['label']\n",
    "\n",
    "# store test x and y data\n",
    "X_test = test_data['text']\n",
    "y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT PREPROCESSING STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "# initalize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# initialize stopwords for the english set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# function to preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # set to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # substitutes all non-letters and whitespaces with an empty character\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # remove punctuation\n",
    "    text = ''.join([word for word in text if word not in string.punctuation])\n",
    "\n",
    "    # tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # utilize stop words\n",
    "    stop_removed_words = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # lemmatize and stem\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stop_removed_words]\n",
    "    stemmed_words = [stemmer.stem(word) for word in lemmatized_words]\n",
    "    \n",
    "    # return the processed text\n",
    "    preprocessed_text = ' '.join(stemmed_words)\n",
    "    return preprocessed_text\n",
    "\n",
    "# apply pre-processing to both the train set and the test set\n",
    "X_train = X_train.apply(preprocess_text)\n",
    "X_test = X_test.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENCODING METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Encoding Method\n",
    "\n",
    "# initialize the tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# vectorize both the train and test set\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Encoding Method\n",
    "\n",
    "# tokenize the text\n",
    "split_text = [sentence.split() for sentence in X_train]\n",
    "\n",
    "# initialize a w2v model\n",
    "w2v_model = Word2Vec(sentences=split_text, vector_size=100, window=5, min_count=2, epochs=50)\n",
    "\n",
    "# method to vectorize each sentence\n",
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if vecs:\n",
    "        return np.mean(vecs, axis=0)\n",
    "    return np.zeros(100)\n",
    "\n",
    "# apply the w2v encoding on both the train and test set\n",
    "X_train_w2v = np.array([vectorize(sentence) for sentence in X_train])\n",
    "X_test_w2v = np.array([vectorize(sentence) for sentence in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: \n",
      "{'C': 10, 'solver': 'liblinear'}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.81      0.91      0.86       789\n",
      "      sexist       0.65      0.45      0.53       297\n",
      "\n",
      "    accuracy                           0.78      1086\n",
      "   macro avg       0.73      0.68      0.69      1086\n",
      "weighted avg       0.77      0.78      0.77      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model with TFIDF\n",
    "\n",
    "# initialize a logistic regressoin model\n",
    "logreg_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# initialize hyperparameters\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# find the most optimal hyperparameters using grid search\n",
    "grid_search = GridSearchCV(logreg_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# prin the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters: \")\n",
    "print(best_params)\n",
    "\n",
    "# set the best estimator\n",
    "best_log_reg = grid_search.best_estimator_\n",
    "\n",
    "# logreg_model.fit(X_train_tfidf, y_train) \n",
    "\n",
    "# predict the data using the most optimal hyperparameters\n",
    "y_pred = best_log_reg.predict(X_test_tfidf)\n",
    "\n",
    "# print a classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# store aspects of the classification report for usage in the evaluation step\n",
    "tfidf_log_precision, tfidf_log_recall, tfidf_log_f1_score, tfidf_log_support = precision_recall_fscore_support(y_test, y_pred)\n",
    "tfidf_log_w_precision, tfidf_log_w_recall, tfidf_log_w_f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: \n",
      "{'C': 1, 'solver': 'liblinear'}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.77      0.95      0.85       789\n",
      "      sexist       0.63      0.23      0.33       297\n",
      "\n",
      "    accuracy                           0.75      1086\n",
      "   macro avg       0.70      0.59      0.59      1086\n",
      "weighted avg       0.73      0.75      0.71      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model with Word2Vec\n",
    "\n",
    "# initialize a logistic regressoin model\n",
    "logreg_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# initialize hyperparameters\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# find the most optimal hyperparameters using grid search\n",
    "grid_search = GridSearchCV(logreg_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_w2v, y_train)\n",
    "\n",
    "# prin the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters: \")\n",
    "print(best_params)\n",
    "\n",
    "# set the best estimator\n",
    "best_log_reg = grid_search.best_estimator_\n",
    "\n",
    "# logreg_model.fit(X_train_tfidf, y_train) \n",
    "\n",
    "# predict the data using the most optimal hyperparameters\n",
    "y_pred = best_log_reg.predict(X_test_w2v)\n",
    "\n",
    "# print a classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# store aspects of the classification report for usage in the evaluation step\n",
    "w2v_log_precision, w2v_log_recall, w2v_log_f1_score, w2v_log_support = precision_recall_fscore_support(y_test, y_pred)\n",
    "w2v_log_w_precision, w2v_log_w_recall, w2v_log_w_f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from previous runs are: Kernal = linear, C = 1. It takes too long to run if I do it every time.\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.82      0.95      0.88       789\n",
      "      sexist       0.75      0.43      0.55       297\n",
      "\n",
      "    accuracy                           0.81      1086\n",
      "   macro avg       0.78      0.69      0.71      1086\n",
      "weighted avg       0.80      0.81      0.79      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM Model with TFIDF\n",
    "\n",
    "# initalize the SVM Model\n",
    "best_svm = SVC(kernel='linear', C=1)\n",
    "\n",
    "# initialize hyperparameters\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100], \n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "# find most optimal hyperparameters using grid search\n",
    "# grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
    "# grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# set the best estimator\n",
    "# best_svm = grid_search.best_estimator_\n",
    "\n",
    "# print the best parameters\n",
    "print(\"Best Parameters from previous runs are: Kernal = linear, C = 1. It takes too long to run if I do it every time.\")\n",
    "\n",
    "\n",
    "# fit the SVM model\n",
    "best_svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# use the SVM model to predict on the test set\n",
    "y_pred = best_svm.predict(X_test_tfidf)\n",
    "\n",
    "# print a classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# store aspects of the classification report for usage in the evaluation step\n",
    "tfidf_svm_precision, tfidf_svm_recall, tfidf_svm_f1_score, tfidf_svm_support = precision_recall_fscore_support(y_test, y_pred)\n",
    "tfidf_svm_w_precision, tfidf_svm_w_recall, tfidf_svm_w_f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from previous runs are: Kernal = rbf. It takes too long to run if I do it every time.\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.78      0.96      0.86       789\n",
      "      sexist       0.72      0.26      0.39       297\n",
      "\n",
      "    accuracy                           0.77      1086\n",
      "   macro avg       0.75      0.61      0.62      1086\n",
      "weighted avg       0.76      0.77      0.73      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM Model with Word2Vec\n",
    "\n",
    "# initalize the SVM Model\n",
    "svm_model = SVC(kernel='rbf')\n",
    "\n",
    "# initialize hyperparameters\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100], \n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "# find most optimal hyperparameters using grid search\n",
    "# grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
    "# grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# set the best estimator\n",
    "# best_svm = grid_search.best_estimator_\n",
    "\n",
    "# print the best parameters\n",
    "print(\"Best Parameters from previous runs are: Kernal = rbf. It takes too long to run if I do it every time.\")\n",
    "\n",
    "# fit the SVM model\n",
    "svm_model.fit(X_train_w2v, y_train)\n",
    "\n",
    "# use the SVM model to predict on the test set\n",
    "y_pred = svm_model.predict(X_test_w2v)\n",
    "\n",
    "# print a classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# store aspects of the classification report for usage in the evaluation step\n",
    "w2v_svm_precision, w2v_svm_recall, w2v_svm_f1_score, w2v_svm_support = precision_recall_fscore_support(y_test, y_pred)\n",
    "w2v_svm_w_precision, w2v_svm_w_recall, w2v_svm_w_f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.82      0.98      0.89       789\n",
      "      sexist       0.91      0.42      0.57       297\n",
      "\n",
      "    accuracy                           0.83      1086\n",
      "   macro avg       0.86      0.70      0.73      1086\n",
      "weighted avg       0.84      0.83      0.81      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest with TFIDF\n",
    "\n",
    "# I attempted to optimize hyperparameters, however it was taking an extremely long time and I found that the performance was sufficient on the basic classifier\n",
    "\n",
    "# initialize random forest\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# fit the random forest\n",
    "random_forest.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# predict using the random forest\n",
    "y_pred_rf = random_forest.predict(X_test_tfidf)\n",
    "\n",
    "# print a classification report\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# store aspects of the classification report for usage in the evaluation step\n",
    "tfidf_rf_precision, tfidf_rf_recall, tfidf_rf_f1_score, tfidf_rf_support = precision_recall_fscore_support(y_test, y_pred_rf)\n",
    "tfidf_rf_w_precision, tfidf_rf_w_recall, tfidf_rf_w_f1_score, _ = precision_recall_fscore_support(y_test, y_pred_rf, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.76      0.96      0.85       789\n",
      "      sexist       0.65      0.22      0.32       297\n",
      "\n",
      "    accuracy                           0.75      1086\n",
      "   macro avg       0.71      0.59      0.59      1086\n",
      "weighted avg       0.73      0.75      0.71      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest with W2V\n",
    "\n",
    "# initialize random forest\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# fit the random forest\n",
    "random_forest.fit(X_train_w2v, y_train)\n",
    "\n",
    "# predict using the random forest\n",
    "y_pred_rf = random_forest.predict(X_test_w2v)\n",
    "\n",
    "# print a classification report\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# store aspects of the classification report for usage in the evaluation step\n",
    "w2v_rf_precision, w2v_rf_recall, w2v_rf_f1_score, w2v_rf_support = precision_recall_fscore_support(y_test, y_pred_rf)\n",
    "w2v_rf_w_precision, w2v_rf_w_recall, w2v_rf_w_f1_score, _ = precision_recall_fscore_support(y_test, y_pred_rf, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RIDGE REGRESSION MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.81      0.93      0.87       789\n",
      "      sexist       0.71      0.43      0.54       297\n",
      "\n",
      "    accuracy                           0.80      1086\n",
      "   macro avg       0.76      0.68      0.70      1086\n",
      "weighted avg       0.78      0.80      0.78      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression Model with TFIDF\n",
    "# not included in evaluation\n",
    "\n",
    "# initialize a ridge regression model and fit it\n",
    "ridge_model = RidgeClassifier()\n",
    "ridge_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# use the ridge regression to predict\n",
    "y_pred_ridge = ridge_model.predict(X_test_tfidf)\n",
    "\n",
    "# store aspects of the classification report for usage in the evaluation step\n",
    "print(\"\\nRidge Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.76      0.95      0.85       789\n",
      "      sexist       0.63      0.22      0.33       297\n",
      "\n",
      "    accuracy                           0.75      1086\n",
      "   macro avg       0.70      0.59      0.59      1086\n",
      "weighted avg       0.73      0.75      0.71      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression Model with Word2Vec\n",
    "# not included in evaluation\n",
    "\n",
    "# initialize a ridge regression model and fit it\n",
    "ridge_model = RidgeClassifier()\n",
    "ridge_model.fit(X_train_w2v, y_train)\n",
    "\n",
    "# use the ridge regression to predict\n",
    "y_pred_ridge = ridge_model.predict(X_test_w2v)\n",
    "\n",
    "# store aspects of the classification report for usage in the evaluation step\n",
    "print(\"\\nRidge Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appreviations: S = sexist, NS = non-sexist, WA = weighted average\n",
      "\n",
      "                Feature + Model  S Precision  S Recall  S F1-score  \\\n",
      "0  TF-IDF + Logistic Regression     0.648780  0.447811    0.529880   \n",
      "1     W2V + Logistic Regression     0.632075  0.225589    0.332506   \n",
      "2                  TF-IDF + SVM     0.752941  0.430976    0.548180   \n",
      "3                     W2V + SVM     0.722222  0.262626    0.385185   \n",
      "4        TF-IDF + Random Forest     0.905109  0.417508    0.571429   \n",
      "5           W2V + Random Forest     0.653061  0.215488    0.324051   \n",
      "\n",
      "   NS Precision  NS Recall  NS F1-score  WA Precision  WA Recall  WA F1-score  \n",
      "0      0.813848   0.447811     0.858683      0.768705   0.782689     0.768762  \n",
      "1      0.765306   0.950570     0.847937      0.728870   0.752302     0.706976  \n",
      "2      0.815502   0.946768     0.876246      0.798393   0.805709     0.786527  \n",
      "3      0.776074   0.961977     0.859083      0.761346   0.770718     0.729481  \n",
      "4      0.817703   0.983523     0.892980      0.841607   0.828729     0.805042  \n",
      "5      0.764170   0.956907     0.849747      0.733784   0.754144     0.705979  \n",
      "\n",
      "Best Performing Model:\n",
      "          Feature + Model  S Precision  S Recall  S F1-score  NS Precision  \\\n",
      "4  TF-IDF + Random Forest     0.905109  0.417508    0.571429      0.817703   \n",
      "\n",
      "   NS Recall  NS F1-score  WA Precision  WA Recall  WA F1-score  \n",
      "4   0.983523      0.89298      0.841607   0.828729     0.805042  \n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "print(\"Appreviations: S = sexist, NS = non-sexist, WA = weighted average\\n\")\n",
    "\n",
    "# set up the data in the table with proper labels\n",
    "data = {\n",
    "    'Feature + Model': ['TF-IDF + Logistic Regression', 'W2V + Logistic Regression',\n",
    "                        'TF-IDF + SVM', 'W2V + SVM',\n",
    "                        'TF-IDF + Random Forest', 'W2V + Random Forest'],\n",
    "    'S Precision':     [tfidf_log_precision[1], w2v_log_precision[1], tfidf_svm_precision[1],\n",
    "                            w2v_svm_precision[1], tfidf_rf_precision[1], w2v_rf_precision[1]],\n",
    "    'S Recall':      [tfidf_log_recall[1], w2v_log_recall[1], tfidf_svm_recall[1],\n",
    "                            w2v_svm_recall[1], tfidf_rf_recall[1], w2v_rf_recall[1]],\n",
    "    'S F1-score':      [tfidf_log_f1_score[1], w2v_log_f1_score[1], tfidf_svm_f1_score[1],\n",
    "                            w2v_svm_f1_score[1], tfidf_rf_f1_score[1], w2v_rf_f1_score[1]],\n",
    "\n",
    "    'NS Precision': [tfidf_log_precision[0], w2v_log_precision[0], tfidf_svm_precision[0],\n",
    "                            w2v_svm_precision[0], tfidf_rf_precision[0], w2v_rf_precision[0]],\n",
    "    'NS Recall':  [tfidf_log_recall[1], w2v_log_recall[0], tfidf_svm_recall[0],\n",
    "                            w2v_svm_recall[0], tfidf_rf_recall[0], w2v_rf_recall[0]],\n",
    "    'NS F1-score':  [tfidf_log_f1_score[0], w2v_log_f1_score[0], tfidf_svm_f1_score[0],\n",
    "                            w2v_svm_f1_score[0], tfidf_rf_f1_score[0], w2v_rf_f1_score[0]],\n",
    "                            \n",
    "    'WA Precision': [tfidf_log_w_precision, w2v_log_w_precision, tfidf_svm_w_precision,\n",
    "                            w2v_svm_w_precision, tfidf_rf_w_precision, w2v_rf_w_precision],\n",
    "    'WA Recall':  [tfidf_log_w_recall, w2v_log_w_recall, tfidf_svm_w_recall,\n",
    "                            w2v_svm_w_recall, tfidf_rf_w_recall, w2v_rf_w_recall],\n",
    "    'WA F1-score':  [tfidf_log_w_f1_score, w2v_log_w_f1_score, tfidf_svm_w_f1_score,\n",
    "                            w2v_svm_w_f1_score, tfidf_rf_w_f1_score, w2v_rf_w_f1_score],\n",
    "}\n",
    "\n",
    "# make the data into dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# print the dataframe\n",
    "print(df)\n",
    "\n",
    "# determine best performing model based on weighted F1 Score\n",
    "best_performance = df[df['WA F1-score'] == df['WA F1-score'].max()]\n",
    "\n",
    "# print the best performing model\n",
    "print(\"\\nBest Performing Model:\")\n",
    "print(best_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7827b394-47a9-4ccc-8ee3-9a9059064cff",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. What preprocessing steps do you follow?\n",
    "   \n",
    "   Your answer: For preprocessing steps, I first convert the text to lowercase in order to make sure there is uniformity in the data. Then, I remove non-alphabetic characters as I found that my models performed better without them included within the data, as well as removing punctuation. I then break the text into tokens and remove common stop words using the NLTK stopwords list. Finally, I lemmatize the words, reducing them to their dictionary form, and then stem them.\n",
    "   \n",
    "2. How do you select the features from the inputs?\n",
    "   \n",
    "   Your answer: The two methods I used to select features were TF-IDF and Word2Vec. For TF-IDF, I run the vectorizer to convert the raw text data into numerical features. For Word2Vec, I tokenize the text, splitting it into a list of words. Then I embed the words with Word2Vec using those tokenized sentences. I specified the parameters to those which I found gave the most optimal results. I then vectorize the sentences, and then perform this vectorization to fit the train data and transform the test data to ensure that there is no test data leaked into the training process. Overall, I noticed Word2Vec was giving me consistently worse results on average then TF_IDF, so all of my best models are with TF-IDF.\n",
    "   \n",
    "3. Which model you use and what is the structure of your model?\n",
    "   \n",
    "   Your answer: The three models I selected were Logisitic Regression, Support Vector Machine, and Random Forest. I also trained a Ridge Regression model, but the performance was not much better than the random forest and logistic regression models. I decided not to include the results in the evaluation so I could keep the table more concise, however the classfication reports and code are still there.\n",
    "\n",
    "   For the logistic regression model, the structure is optimized using hyperparameters, with the 'C' parameter being particularly important for achieving the highest possible accuracy. I also optimized the 'solver' parameter.\n",
    "   \n",
    "   For the support vector machine model, the structure uses a linear kernal due to giving the best performance amongst other kernals.\n",
    "\n",
    "   For the random forest model, the structure uses the default for random forest, since I found this not only gave the best performance, but attempting optimize hyperparameters was struggling to run on my computer.\n",
    "\n",
    "   Overall, random forest ran the best with TF-IDF when compared with the other models and feature selection methods.\n",
    "   \n",
    "4. How do you train your model?\n",
    "   \n",
    "   Your answer: All three models are trained using the training set with each of the two feature selection methods (TF-IDF and Word2Vec):\n",
    "\n",
    "   For the logistic regression model, it is trained by fitting the data with different combinations of hyperparameters to find the best model to be selected. The best estimator is then chosen based on the scoring method that I specified. It then makes its prediction on the test set.\n",
    "\n",
    "   For the support vector machine model, it trained by fitting the data in a linear kernal and then predicting on the test set.\n",
    "\n",
    "   For the random forest model, it is trained similarly to the other two models.\n",
    "   \n",
    "5. What is the performance of your best model?\n",
    "   \n",
    "   Your answer: The best performing model was TF-IDF + Random Forest, which gave me the highest Weighted Average F1-Score as shown above in the evaluation section. It shows higher precision for the 'not sexist' class than the 'sexist' class, which was typical between all of my models. Since 'not sexist' has a high recall with this model, it indicates that it correctly identifies a majority of those cases.\n",
    "   \n",
    "6. What other models or feature engineering methods would you like to implement in the future?\n",
    "   \n",
    "   Your answer: I would have liked to try using a pre-trained BERT-based model, specifically DeBERTaV3, however I was not able to figure out how to get it reliably working and I found it to be potentially too advanced for my current understanding of these topics. From my research into optimal models and methods to use, I found that it was a popular choice and likely could have given better results than the ones that I found using my models. I was really curious to see how something more advanced would perform on this data. It would also be interesting how other models would have done with the data, such as a neural network or more complex options which may be able to capture necessary information more efficiently.\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
